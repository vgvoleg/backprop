{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK7GyR-2K4eJ",
    "colab_type": "text"
   },
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdEY6TrrK7cs",
    "colab_type": "text"
   },
   "source": [
    "Дана двухслойная нейронная сеть:  \n",
    "\n",
    "<center> $x = input$  </center>\n",
    "<center> $z = W_{h}x$ </center>  \n",
    "<center> $h = ReLU(z)$  </center> \n",
    "<center> $\\theta = W_{o}h$   </center> \n",
    "<center> $\\hat{y} = softmax(\\theta)$  </center> \n",
    "\n",
    "\n",
    "<center> $E = CE(\\hat{y}, y)$ </center>\n",
    "\n",
    "\n",
    "\n",
    "Вычислять производные будем с помощью chain rule:\n",
    "\n",
    "<center> $\\frac{\\partial E}{\\partial W_{o}} = \\frac{\\partial E}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial \\theta}\\frac{\\partial \\theta}{\\partial W_{o}}$ </center> \n",
    "\n",
    "Введем вспомогательные дельты:\n",
    "\n",
    "<center> $\\delta_1 = \\frac{\\partial E}{\\partial \\theta}$ , $\\delta_2 = \\frac{\\partial E}{\\partial z}$ </center>\n",
    "\n",
    "\n",
    "<center> $\\delta_1 = \\frac{\\partial E}{\\partial \\theta} = (\\hat{y} - y)^T$ </center> \n",
    "\n",
    "<center> $\\delta_2 = \\frac{\\partial E}{\\partial \\theta}\\frac{\\partial \\theta}{\\partial h}\\frac{\\partial h}{\\partial z} = \\delta_1 \\frac{\\partial \\theta}{\\partial h}\\frac{\\partial h}{\\partial z} = \\delta_1 W_{o} \\frac{\\partial h}{\\partial z} = \\delta_1 W_{o} \\circ ReLU'(z) = \\delta_1 W_{o} \\circ sgn(h)$ </center> \n",
    "\n",
    "Вычисляем производные: \n",
    "\n",
    "<center> $\\frac{\\partial E}{\\partial W_{o}} = \\frac{\\partial E}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial W_{o}} = \\delta_1 \\frac{\\partial \\theta}{\\partial W_{o}} = \\delta_1^T h^T$ </center> \n",
    "\n",
    "<center> $\\frac{\\partial E}{\\partial W_{h}} = \\frac{\\partial E}{\\partial z}\\frac{\\partial z}{\\partial W_{h}} = \\delta_2 \\frac{\\partial z}{\\partial W_{h}} = \\delta_2^T x^T$ </center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QaEYtkZ8Z-ep",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJqC1Y6cLNYT",
    "colab_type": "text"
   },
   "source": [
    "### Activation and loss functions\n",
    "\n",
    "Определяем функции активации - softmax и relu, а так же функцию потерь - cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yJ8eEfkuae7K",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    tmp = x - x.max(axis=1, keepdims=True)\n",
    "    np.exp(tmp, out=x)\n",
    "    x /= x.sum(axis=1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    out = x[:]\n",
    "    out[out <= 0] = 0\n",
    "    out[out > 0] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def crossentropy_loss(y_true, y_prob):\n",
    "    return - xlogy(y_true, y_prob).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppb3XwCPLekL",
    "colab_type": "text"
   },
   "source": [
    "### DNNClassifier definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rD5mXjrtGWqv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class DNNClassifier():\n",
    "    def __init__(self, hidden_layer_sizes=(100,),\n",
    "                 activation_functions=(None, None),\n",
    "                 loss_function=crossentropy_loss,\n",
    "                 batch_size=1, learning_rate=0.001,\n",
    "                 max_iter=200, random_state=42, verbose=False):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self._label_binarizer = LabelBinarizer()\n",
    "\n",
    "    def __forward_layer(self, x, w, activation_function=None):\n",
    "        out = np.dot(x, w)\n",
    "        if activation_function:\n",
    "            out = activation_function(out)\n",
    "        return out\n",
    "\n",
    "    def __forward_propagate(self, x):\n",
    "        activations = [x]\n",
    "        for next_layer, activation in zip(self.weights, self.activation_functions):\n",
    "            out = self.__forward_layer(activations[-1], next_layer, activation)\n",
    "            activations.append(out)\n",
    "        return activations\n",
    "\n",
    "    def __back_propagation(self, activations, y):\n",
    "        weights = self.weights\n",
    "        gradients = [np.empty_like(layer) for layer in weights]\n",
    "\n",
    "        deltas1 = activations[2] - y # 256,10\n",
    "        gradients[1] = np.dot(activations[1].T, deltas1) # 128,10\n",
    "\n",
    "        deltas2 = np.dot(deltas1, weights[1].T) * relu_derivative(activations[1]) # 256,128\n",
    "\n",
    "        gradients[0] = np.dot(activations[0].T, deltas2) # hotim 784,128\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def __init_layer(self, input_size, output_size):\n",
    "        a = 2.0 / (input_size + output_size)\n",
    "        w = np.random.uniform(-a, a, (input_size, output_size))\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y, shuffle=False):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        y_train = y\n",
    "        X_train = X\n",
    "        y = self._label_binarizer.fit_transform(y)\n",
    "        num_classes = len(self._label_binarizer.classes_)\n",
    "\n",
    "        n, p = X.shape\n",
    "        s = self.hidden_layer_sizes[0]\n",
    "\n",
    "        self.weights = [\n",
    "            self.__init_layer(p, s),\n",
    "            self.__init_layer(s, num_classes)\n",
    "        ]\n",
    "\n",
    "        for j in range(self.max_iter):\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "            if shuffle:\n",
    "                indices = np.arange(n)\n",
    "                np.random.shuffle(indices)\n",
    "                X = X.take(indices, axis=0)\n",
    "                y = y.take(indices, axis=0)\n",
    "\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                X_batch = X[i: i + self.batch_size]\n",
    "                y_batch = y[i: i + self.batch_size]\n",
    "\n",
    "                activations = self.__forward_propagate(X_batch)\n",
    "\n",
    "                y_prob = activations[-1]\n",
    "\n",
    "                accumulated_loss += self.loss_function(y_batch, y_prob)\n",
    "                gradients = self.__back_propagation(activations, y_batch)\n",
    "\n",
    "                gradients = [gradient / self.batch_size for gradient in gradients]\n",
    "                self.weights = [weight - self.learning_rate * grad for weight, grad in\n",
    "                                zip(self.weights, gradients)]\n",
    "\n",
    "            if self.verbose:\n",
    "                loss = accumulated_loss / X.shape[0]\n",
    "                y_pred = self.predict(X_train)\n",
    "                accuracy = (y_pred == y_train).mean()\n",
    "                print(\"Epoch {}/{};\\t Train accuracy: {:.3f} \\t Loss : {:.3f}\".format(j + 1, self.max_iter, accuracy,\n",
    "                                                                                      loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self.__forward_propagate(X)\n",
    "        y_pred = activations[-1]\n",
    "        return self._label_binarizer.inverse_transform(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz1TdS9yL3q7",
    "colab_type": "text"
   },
   "source": [
    "### Load and prepare dataset\n",
    "\n",
    "Данные представлены в виде матриц 28*28, для дальнейшего использования преобразовываем в вектор и нормализуем, поделив на 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1kI_hJkkGa5f",
    "colab_type": "code",
    "outputId": "df94e615-64a3-4571-a2e1-d772c91fbcbb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "train size:  (60000, 784) (60000,)\n",
      "test size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('train size: ', x_train.shape, y_train.shape)\n",
    "print('test size: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf64uZmvMGRW",
    "colab_type": "text"
   },
   "source": [
    "### Instantiate and fit DNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3-7OsY3QMDl-",
    "colab_type": "code",
    "outputId": "cf61be0a-5057-4e1a-c954-5656f963852a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50;\t Train accuracy: 0.923 \t Loss : 0.565\n",
      "Epoch 2/50;\t Train accuracy: 0.950 \t Loss : 0.211\n",
      "Epoch 3/50;\t Train accuracy: 0.963 \t Loss : 0.152\n",
      "Epoch 4/50;\t Train accuracy: 0.971 \t Loss : 0.120\n",
      "Epoch 5/50;\t Train accuracy: 0.975 \t Loss : 0.100\n",
      "Epoch 6/50;\t Train accuracy: 0.978 \t Loss : 0.085\n",
      "Epoch 7/50;\t Train accuracy: 0.981 \t Loss : 0.074\n",
      "Epoch 8/50;\t Train accuracy: 0.982 \t Loss : 0.066\n",
      "Epoch 9/50;\t Train accuracy: 0.984 \t Loss : 0.059\n",
      "Epoch 10/50;\t Train accuracy: 0.986 \t Loss : 0.053\n",
      "Epoch 11/50;\t Train accuracy: 0.987 \t Loss : 0.048\n",
      "Epoch 12/50;\t Train accuracy: 0.988 \t Loss : 0.043\n",
      "Epoch 13/50;\t Train accuracy: 0.989 \t Loss : 0.039\n",
      "Epoch 14/50;\t Train accuracy: 0.990 \t Loss : 0.036\n",
      "Epoch 15/50;\t Train accuracy: 0.991 \t Loss : 0.032\n",
      "Epoch 16/50;\t Train accuracy: 0.992 \t Loss : 0.030\n",
      "Epoch 17/50;\t Train accuracy: 0.992 \t Loss : 0.027\n",
      "Epoch 18/50;\t Train accuracy: 0.993 \t Loss : 0.025\n",
      "Epoch 19/50;\t Train accuracy: 0.994 \t Loss : 0.023\n",
      "Epoch 20/50;\t Train accuracy: 0.994 \t Loss : 0.021\n",
      "Epoch 21/50;\t Train accuracy: 0.995 \t Loss : 0.019\n",
      "Epoch 22/50;\t Train accuracy: 0.995 \t Loss : 0.018\n",
      "Epoch 23/50;\t Train accuracy: 0.995 \t Loss : 0.016\n",
      "Epoch 24/50;\t Train accuracy: 0.996 \t Loss : 0.015\n",
      "Epoch 25/50;\t Train accuracy: 0.996 \t Loss : 0.014\n",
      "Epoch 26/50;\t Train accuracy: 0.996 \t Loss : 0.013\n",
      "Epoch 27/50;\t Train accuracy: 0.997 \t Loss : 0.012\n",
      "Epoch 28/50;\t Train accuracy: 0.997 \t Loss : 0.011\n",
      "Epoch 29/50;\t Train accuracy: 0.997 \t Loss : 0.010\n",
      "Epoch 30/50;\t Train accuracy: 0.997 \t Loss : 0.010\n",
      "Epoch 31/50;\t Train accuracy: 0.998 \t Loss : 0.009\n",
      "Epoch 32/50;\t Train accuracy: 0.998 \t Loss : 0.009\n",
      "Epoch 33/50;\t Train accuracy: 0.998 \t Loss : 0.008\n",
      "Epoch 34/50;\t Train accuracy: 0.998 \t Loss : 0.008\n",
      "Epoch 35/50;\t Train accuracy: 0.998 \t Loss : 0.007\n",
      "Epoch 36/50;\t Train accuracy: 0.999 \t Loss : 0.007\n",
      "Epoch 37/50;\t Train accuracy: 0.999 \t Loss : 0.006\n",
      "Epoch 38/50;\t Train accuracy: 0.999 \t Loss : 0.006\n",
      "Epoch 39/50;\t Train accuracy: 0.999 \t Loss : 0.006\n",
      "Epoch 40/50;\t Train accuracy: 0.999 \t Loss : 0.005\n",
      "Epoch 41/50;\t Train accuracy: 0.999 \t Loss : 0.005\n",
      "Epoch 42/50;\t Train accuracy: 1.000 \t Loss : 0.005\n",
      "Epoch 43/50;\t Train accuracy: 1.000 \t Loss : 0.005\n",
      "Epoch 44/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 45/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 46/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 47/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 48/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 49/50;\t Train accuracy: 1.000 \t Loss : 0.004\n",
      "Epoch 50/50;\t Train accuracy: 1.000 \t Loss : 0.003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DNNClassifier at 0x7fd8364392e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_functions = (relu, stable_softmax)\n",
    "estimator = DNNClassifier(hidden_layer_sizes=(128,),\n",
    "                          activation_functions=activation_functions,\n",
    "                          batch_size=256,\n",
    "                          learning_rate=0.5, max_iter=50,\n",
    "                          random_state=42, verbose=True)\n",
    "\n",
    "estimator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oBeYkZn8Irm7",
    "colab_type": "code",
    "outputId": "9225309e-8b0b-4f65-9ea0-a896c05296b1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.9797 \n"
     ]
    }
   ],
   "source": [
    "y_pred = estimator.predict(x_test)\n",
    "print(\"Accuracy on test dataset: %s \" % (y_pred == y_test).mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "backprob.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
